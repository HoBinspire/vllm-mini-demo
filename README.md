# vLLM ç®€ä»‹
> vLLM ä¸æ”¯æŒåŸç”Ÿ windows ç³»ç»Ÿ.

## vLLM æ˜¯ä»€ä¹ˆï¼Ÿä¸€å¥è¯æ¦‚æ‹¬
vLLM æ˜¯ä¸€ä¸ªæŠŠå¤§æ¨¡å‹æ¨ç†åšæˆâ€œé«˜æ€§èƒ½ã€ä½æˆæœ¬ã€æ˜“è½åœ°â€çš„å¼€æºå¼•æ“â€”â€”å®ƒèƒ½è®© Llama-3-70B è¿™æ ·çš„å·¨å…½åœ¨å•å¼  A100 ä¸Šè·‘å‡º 10Ã— äºä¼ ç»Ÿæ¡†æ¶çš„ **QPS**ï¼Œå¹¶ä¸”ä¸€æ¡å‘½ä»¤å°±èƒ½å¼€æˆå…¼å®¹ OpenAI çš„ API æœåŠ¡ã€‚

## 1. è¯ç”ŸèƒŒæ™¯
- **å‘èµ·æ–¹**ï¼šUC Berkeley Sky Computing Labï¼ˆ2023 å¼€æºï¼‰ã€‚
- **åˆè¡·**ï¼šè§£å†³ LLM éƒ¨ç½²çš„ä¸‰å¤§ç—›ç‚¹â€”â€”==**GPU å†…å­˜æµªè´¹ã€è¯·æ±‚å»¶è¿Ÿé«˜ã€ååé‡ä½**==ã€‚

## 2. æŠ€æœ¯æ ¸å¿ƒï¼šPagedAttention
- **åŸç†**ï¼šæŠŠ KV-Cache æ‹†æˆå›ºå®šå¤§å°çš„â€œé¡µâ€ï¼Œåƒæ“ä½œç³»ç»Ÿä¸€æ ·åŠ¨æ€æ˜ å°„ï¼Œå†…å­˜ç¢ç‰‡é™ä½ 3-5 å€ï¼Œ==**æ˜¾å­˜åˆ©ç”¨ç‡æå‡ 3-5 å€**==ã€‚
- **æ•ˆæœ**ï¼šåŒæ ·æ˜¾å­˜ï¼Œ==**å¹¶å‘é‡æå‡ 10Ã—**==ï¼›åŒæ ·å¹¶å‘ï¼Œ==æ˜¾å­˜èŠ‚çœ 60%==ã€‚

> ä¸€å¥è¯æ€»ç»“ï¼š
> vLLM = PagedAttention + è¿ç»­æ‰¹å¤„ç† + é‡åŒ–/å¹¶è¡Œ

## 3. å…³é”®ç‰¹æ€§é€Ÿè§ˆ

| ç‰¹æ€§           | ä¸€å¥è¯è¯´æ˜                                                         |
|----------------|--------------------------------------------------------------------|
| è¿ç»­æ‰¹å¤„ç†     | è¯·æ±‚éšåˆ°éšç®—ï¼Œé¿å…â€œæ•´æ‰¹ç­‰æœ«å°¾â€                                     |
| åˆ†å¸ƒå¼å¹¶è¡Œ     | åŸç”Ÿæ”¯æŒå¼ é‡å¹¶è¡Œ & æµæ°´çº¿å¹¶è¡Œï¼Œ70B å¯è·¨ 8Ã—A100 çº¿æ€§æ‰©å±•            |
| é‡åŒ–å‹å¥½       | å†…ç½® GPTQ/AWQ/FP8ï¼ŒINT4 ä¸‹ 70B èƒ½åœ¨ 24 GB æ˜¾å­˜è·‘                   |
| å¤š LoRA        | åŒä¸€å¼ å¡åŒæ—¶åŠ è½½ N ä¸ª LoRA æƒé‡ï¼ŒæœåŠ¡å¤šç§Ÿæˆ·                        |
| OpenAI å…¼å®¹    | vllm serve å³å¼€å³ç”¨ï¼ŒSDK æ— éœ€æ”¹ä¸€è¡Œä»£ç                             |

## 4. vLLM çš„åˆ›æ–°ç‚¹

### æ˜¾å­˜å±‚ï¼šPagedAttentionï¼ˆâ‰  å•çº¯åˆ†é¡µï¼‰
- **ä¼ ç»Ÿåšæ³•**ï¼šä¸€æ¬¡æ€§ä¸ºæ¯ä¸ªåºåˆ—ç”³è¯·è¿ç»­çš„ KV Cache åŒºåŸŸ â†’ åºåˆ—é•¿åº¦æœªçŸ¥ â†’ åªèƒ½æŒ‰æœ€å¤§é•¿åº¦é¢„ç•™ â†’ 80% ä»¥ä¸Šæ˜¾å­˜è¢«æµªè´¹ã€‚
- **vLLM åšæ³•**ï¼šæŠŠ KV Cache åˆ‡æˆå›ºå®šå¤§å°çš„ blockï¼Œç”¨é€»è¾‘â†’ç‰©ç†æ˜ å°„è¡¨ï¼ˆBlock Tableï¼‰åŠ¨æ€åˆ†é…ï¼›
  - åŒä¸€å—ç‰©ç†æ˜¾å­˜å¯è¢«ä¸åŒåºåˆ—å…±äº«ï¼ˆprompt ç›¸åŒå‰ç¼€æ—¶ï¼‰ï¼›
  - å½“åºåˆ—åˆ†å‰æ—¶ï¼Œå†è§¦å‘ copy-on-write å¤åˆ¶æ–°å—ï¼›
  - æ˜¾å­˜ç¢ç‰‡ä» 50% é™åˆ° <4%ã€‚
- è¿™ä¸ä»…æ˜¯â€œåˆ†é¡µâ€ï¼Œè€Œæ˜¯å¼•å…¥äº†**å¼•ç”¨è®¡æ•° + å†™æ—¶å¤åˆ¶ + å…±äº«**çš„å®Œæ•´å†…å­˜ç®¡ç†ç­–ç•¥ã€‚

### è°ƒåº¦å±‚ï¼šToken-level å¾®è°ƒåº¦å™¨
- **è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰**ï¼š
  - ä¼ ç»Ÿæ¡†æ¶è¦ç­‰æ•´æ‰¹åºåˆ—éƒ½ç»“æŸæ‰æ¢ä¸‹ä¸€æ‰¹ï¼›vLLM æ¯ç”Ÿæˆä¸€ä¸ª token å°±é‡æ–°è¯„ä¼°æ˜¾å­˜ï¼ŒæŠŠæ–°è¯·æ±‚æˆ–å·²å®Œæˆçš„åºåˆ—å³æ—¶å¡è¿› GPUï¼Œååé‡æå‡ 8â€“24Ã—ã€‚
- **æŠ¢å /æ¢å¤æœºåˆ¶**ï¼š
  - æ˜¾å­˜åƒç´§æ—¶ï¼ŒvLLM å¯ä»¥æŠŠååˆ°è¯·æ±‚çš„ KV block æ¢å‡ºåˆ° CPUï¼ˆç±»ä¼¼ OS swapï¼‰ï¼Œå®Œæˆåæ¢å›æ¥ï¼Œä¿è¯é«˜ä¼˜å…ˆçº§è¯·æ±‚å…ˆå®Œæˆã€‚

### æ‰§è¡Œå±‚ï¼šå®šåˆ¶åŒ– CUDA Kernel & å¹¶è¡Œ
- è‡ªç ” PagedAttention CUDA kernelï¼ŒåŸç”Ÿæ”¯æŒéè¿ç»­åœ°å€ gather/scatterï¼›
- æ”¯æŒ Tensor Parallel + Pipeline Parallelï¼Œ70B æ¨¡å‹å¯åœ¨å¤šå¡çº¿æ€§æ‰©å±•ï¼›
- é›†æˆ FlashAttention-2ã€GPTQ/AWQ é‡åŒ–ã€LoRA çƒ­æ’æ‹”ç­‰å·¥ç¨‹ä¼˜åŒ–ã€‚

# 5.æœ€å°demoç¤ºä¾‹
## ğŸ“ é¡¹ç›®ç›®å½•
```
vllm-mini-demo/
â”œâ”€â”€ app.py               # å¯åŠ¨æœåŠ¡
â”œâ”€â”€ client.py            # å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹
â”œâ”€â”€ requirements.txt     # ä¾èµ–
â”œâ”€â”€ .env.example         # å¯é€‰ï¼šæ”¾æ¨¡å‹è·¯å¾„
â””â”€â”€ README.md            # æœ¬é¡µç¼©å‡ç‰ˆ
```

## 1ï¸âƒ£ requirements.txt
```Text
vllm>=0.4.2          # ä¸»æ¡†æ¶
openai>=1.0          # å®¢æˆ·ç«¯ SDK
python-dotenv>=1.0   # è¯»å– .envï¼ˆå¯é€‰ï¼‰
```

ä¸€é”®å®‰è£…ä¾èµ–ï¼š
```bash
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
```

## 2ï¸âƒ£ app.py ï¼ˆ3 è¡Œæ ¸å¿ƒï¼Œå…¶ä½™æ˜¯æ—¥å¿—ï¼‰
```python
# app.py
import os, uvicorn, logging
from vllm import LLM, SamplingParams
from vllm.entrypoints.openai.api_server import build_app

# å¦‚æœä½ æƒ³æ¢æ¨¡å‹ï¼Œåœ¨è¿™é‡Œæ”¹
MODEL = os.getenv("MODEL", "microsoft/DialoGPT-small")   # å°æ¨¡å‹å¿«ä¸‹è½½
llm_engine = LLM(model=MODEL, tensor_parallel_size=1)    # å•å¡å³å¯

# æ„å»º FastAPI åº”ç”¨
app = build_app(llm_engine)

if __name__ == "__main__":
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
```

## 3ï¸âƒ£ client.py ï¼ˆ1 è¡Œè°ƒç”¨ï¼‰
```python
# client.py
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")

resp = client.chat.completions.create(
    model="microsoft/DialoGPT-small",
    messages=[{"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç» vLLM"}],
    max_tokens=50
)
print(resp.choices[0].message.content)
```

## 4ï¸âƒ£ å¯åŠ¨ & éªŒè¯
```bash
# 1. å¯åŠ¨æœåŠ¡ï¼ˆé¦–æ¬¡ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼‰
python app.py
# 2. å¦å¼€ç»ˆç«¯
python client.py
```
ç»ˆç«¯ä¼šè¾“å‡º vLLM ç”Ÿæˆçš„å›ç­”ï¼Œä¾‹å¦‚ï¼š
vLLM æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€æ˜“éƒ¨ç½²çš„å¤§æ¨¡å‹æ¨ç†å¼•æ“ã€‚

## 5ï¸âƒ£ ä½“éªŒå¹¶å‘ï¼ˆå¯é€‰ï¼‰
```bash
# 50 å¹¶å‘è¯·æ±‚ï¼Œçœ‹ååé‡
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"microsoft/DialoGPT-small","messages":[{"role":"user","content":"Hi"}],"max_tokens":32}' \
  &  # å¤åˆ¶å¤šæ¡å³å¯
```

## 6ï¸âƒ£ ä¸€é”®è„šæœ¬ï¼ˆæ‡’äººç‰ˆï¼‰
```bash
git clone https://github.com/yourname/vllm-mini-demo.git
cd vllm-mini-demo
python app.py
```